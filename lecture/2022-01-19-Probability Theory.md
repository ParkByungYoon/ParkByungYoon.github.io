---
layout: single
title: '딥러닝에서의 확률론 맛보기'
---

## 딥러닝에서의 확률론
- 딥러닝은 확률론 기반의 기계학습 이론에 바탕
- loss function의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도
	- L2 norm의 경우 예측 오차의 분산을 가장 최소화
	- cross-entropy loss는 모델 예측의 불확실성을 최소화
	- 기계학습에서의 loss function은 데이터에서의 분포와 모델 예측의 분포 간 차이를 최소화하는 방향으로 확률론에 기반하여 유도된다
- 분산 및 불확실성을 측정하는 방법을 알아야한다
- 확률분포는 데이터의 초상화
	- $X$ X $Y$ 는 데이터 공간
	- $D$ 는 확률분포
	- 데이터만을 가지고 확률 분포 $D$를 아는 것은 불가능하다
	- 데이터는 확률변수로 $(x,y)$ ~ $D$ 라 표기
	- $(x,y)$ 확률변수는 데이터 공간 상에서 관측 가능한 데이터 ($X$ X $Y$ 의 원소)
	- 확률변수는 임의로 데이터 공간에서 관측을 하게 되는 함수로 이해하여 데이터 추출 시 확률변수를 이용한다
	- 확률변수는  **데이터공간 $X$ X $Y$ 가 아닌** 확률분포 $D$에 따라 discrete와 continuous 확률변수로 구분된다
- 이산확률변수 vs 연속확률변수
	1. 이산확률변수
		- 확률변수가 가질 수 있는 모든 경우의 수를 고려하여 확률을 더해 모델링
		- $P(X=x)$는 확률변수가 x값을 가질 확률로 해석한다 (확률질량함수)
	2. 연속확률변수
		- 반면에 연속형 확률변수는 확률변수가 x값을 가질 확률을 구하는 것은 불가능하다 
		- 연속형 확률변수는 데이터 공간에 정의된 확률변수의 **밀도** 위에서의 적분을 통해 모델링
		- $P(X\in A) = \int_A{P(x)dx}$ 는 X가 집합 A에 속할 확률을 계산하는데 A상에서 $P(x)$라는 함수를 적분하는 형태로 모델링한다
		- 여기서 $P(x)$ 함수는 누적확률분포의 변화율을 모델링한 개념 (밀도함수이며 확률 X)
	3. 모든 확률 변수가 두가지로만 구분되지는 않는다
- 주변확률분포 vs 결합확률분포
	- 결합분포 $P(x,y)$는 $D$를 모델링한다 (확률분포 $D$는 결합분포 $P(x,y)$ 모델링 방법에 따라 이산형/연속형 바꿀 수 있다)
	- $P(x)$는 $y$에 대한 정보가 담겨있진 않다 (주변확률분포)
	- 그렇기 때문에 각각의 y에 대한 결합확률분포를 더하거나 적분을 통해 주변확률분포를 유도해 낼 수 있다
- 조건부 확률 분포
	-  $P(x|y)$는 y가 주어진 상황에서 x에 대한 분포를 말한다 (입력 x와 출력 y 사이의 관계를 모델링)
	- 특정 클래스가 주어진 조건에서의 데이터 확률분포
	- $P(y|x)$는 입력변수 x에 대해 정답이 y일 확률을 의미
- 확률분포는 데이터를 바라보는 데에 주요한 도구로서 사용된다
- 선형모델과 softmax 함수의 결합은 데이터에서 추출된 패터을 기반으로 확률 해석하는데 사용된다
	- 데이터에서 추출된 패턴 ($\phi(x)$)과 가중치행렬 $W$를 softmax에 넘김을 통해 확률을 구하는 것과 $P(y|x)$를 계산하는 것을 똑같이 이해할수있다
- 회귀 문제의 경우 연속성을 띄기 때문에 확률로 다루기에는 어려움이 있다
	- 회귀 문제의 경우 조건부기대값 $E[y|x]$을 추정한다
		- Why? 조건부기댓값은 $E||y-f(x)||_2$을 최소화하는 함수 f(x)와 일치하기 때문 (예측 오차의 분산을 최소화)
	- 조건부 밀도함수 $P(x|y)$와 $y$를 곱해 적분함을 통해 계산
	- 원하는 목적에 따라 estimator가 달라질 수 있다 ex) 조건부기댓값이 아닌 median 또한 쓸수있다
- 기댓값이란?
	- 데이터를 대표하는 통계량
	- 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용된다
	- 주어진 함수에 확률밀도 함수를 곱해 적분/급수를 사용함을 통해 구한다
	- 분산, 첨도, 공분산 등 여러 통계량 계산이 가능하다

- 몬테카를로 샘플링
	- 기계학습의 많은 문제들은 확률분포를 명시적으로 모른다
	- 이러한 이유로 데이터를 이용해 기대값을 계산할 필요가 있는데 이때 사용되는 기법이 몬테카를로 샘플링이다
	- 타겟으로하는 $f(x)$에 sampling한 데이터를 대입 한 후에 $f(x)$의 산술평균을 구해주게 된다면 그 값이 기댓값에 근사하게 된다
	- sampling만 독립적으로 진행된다면 대수의 법칙에 의해 수렴성이 보장되어 몬테카를로 샘플링이 성립할 수 있다