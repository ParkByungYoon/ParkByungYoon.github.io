---
layout: single
title: '모수 추정 MLE 알아보기'
categories:
  - boostcamp AI tech
---

## 모수 추정
- 통계적 모델링이란 적절한 가정위에서 확률분포를 추정하는 것
- 데이터만으로 정확하게 알아내는 것은 불가능하고 근사적으로 확률분포를 추정
- 즉 분포를 정확하게 맞추는 것보다 데이터와 추정방법의 불확실성을 고려해 위험을 최소화하는 것
- parametric 방법론
	- 데이터가 특정 확률분포를 따른다고 a priori 가정한 후 그 분포를 결정하는 parameter를 추정하는 방법
	- 정규분포 (평균, 분산 --> 모수) / 평균과 분산 (parameter)을 추정하여 데이터를 학습
- non-parametric 방법론
	- 특정 확률 분포 가정 X
	- 모수가 무한히 많거나 / 모수가 데이터 개수에 따라 바뀌는 경우
	- **모수를 쓰지 않는 것이 아니다**

- 확률분포 가정하기
	- 히스토그램/통계치를 통해 데이터 전체적인 모양 관찰
		- 데이터가 2개의 값 --> 베르누이 분포
		- n개의 이상적인 값 --> 카테고리 분포
		- [0,1] 사이에서 값 --> 베타 분포
		- 0이상의 값 --> 감마분포, 로그정규분포 등
		- 실수 전체에서 값 --> 정규분포, 라플라스 분포
	- **기계적인 확률분포 가정은 금물, 데이터를 생성하는 원리를 먼저 고려하는 것이 원칙 (통계적 검정이 필요하다)**
- 데이터로 모수를 추정해보자
	- 정규분포 모수는 $\mu$와 $\sigma$로 추정하는 통계량 (statistic)은 다음과 같다

	- 표본 평균(주어진 데이터의 산술평균)의 기댓값은 모집단의 평균과 일치하다
	 	- $\overline{X} = \frac{1}{N} \sum_{i=1}^N{X_i}$
		- $E[\overline{X}] = \mu$
	- 표본 분산(주어진 데이터에서 표본평균을 빼 제곱한 값의 산술평균)의 기댓값은 모집단의 분산과 일치하다
		- $S^2$= $\frac{1}{N-1} \sum_{i=1}^N{(X_i - \overline{X})^2}$
		- $E[S^2] = \sigma^2$ 
	- **N이 아니라 N-1로 나누는 이유는 unbiased  추정량을 구하기 위해서 (기댓값을 취했을 때 원래 모집단의 통계치와 일치하기 위해 사용)**
- 표본들의 분포 (sample distribution), 표본 평균과 표본 분산의 확률분포 (sampling distribution)
	- 통계량의 확률분포를 sampling distribution 이라 부르며 표본평균의 sampling distribution은 N이 커질수록 정규분포를 따른다
	- sample distribution은 정규분포를 따르지 않는다면 데이터가 많아도 정규분포가 될수 없다 
	- sampling distribution의 경우 모집단의 분포가 정규분포를 따르지 않아도 중심극한정리(Central Limit Theorem)에 의해 성립한다
	- 데이터를 모으면 모을수록 정규분포로 향하며 평균값은 하나의 값으로 모이는 반면 분산이 점점 작아지는 것을 확인할 수 있다.

## 최대가능도 추정법
- 확률분포마다 사용하는 모수가 다르기 때문에 표본평균이나 표본분산만을 가지고 확률분포를 추정하는 것은 위험하다
- Maximum Likelihood Estimation, MLE 는 이론적으로 가장 높은 모수를 추정하는 방법 중 하나이다
- 가능도함수(likelihood)
	- $P(x|\theta)$ 확률 밀도함수와 같다 하지만 관점의 차이가 있다 
	- 데이터가 주어진 상황에서 $\theta$가 값이 변경됨에 따라 값이 바뀌는 함수
	- 모수 $\theta$를 따르는 분포가 데이터$x$를 관찰할 가능성을 뜻한다 
	- $\theta$에 대해서 다 더했을 때 1이되는 개념이 아니기 때문에 확률이라 보기에는 어렵고, $\theta$에 따라 크고 작음 (대소 관계)이 정해질 수 있는 함수 정도로 이해해야한다
	- 데이터 집합 $X$ 행 벡터(sample)가 **독립적으로**추출되었을 경우 확률 질량 함수들의 곱으로 표현이 가능하다
		- log 함수의 성질을 이용해 덧셈으로 바꿀 수 있다
	- 왜 log likelihood를 사용하는가
		- log를 사용하지 않아도 MLE가 가능하다
		- 하지만 데이터 숫자가 많아지는 경우 컴퓨터의 정확도로 likelihood 계산하는 것은 어렵다 (computational cost)
		- 경사하강법으로 가능도를 최적화하는 경우 미분 연산 시 로그 가능도를 사용하는 경우 연산량을 $O(n^2)$에서 $O(n)$으로 줄어든다
		- 대게의 손실함수의 경우 경사하강법을 사용해 음의 가능도를 최적화한다

## 확률분포 거리
- 손실함수들은 데이터 공간에 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포 두개 사이의 거리를 통해 유도된다
- 확률 분포 $P(x)$, $Q(x)$ 사이의 거리를 구하는 방법 중Kullback-Leibler Divergence를 살펴보겠다
	- KL Divergence
		- $\log{Q(x)}$에 대한 기대값(크로스 엔트로피)과 $\log{P(x)}$에 대한 기대값(엔트로피)을 더해주는 형태
		- 두개의 엔트로피 함수로 표현된다
		- MLP 손실함수에서 사용되는 최대가능도 추정법이정답레이블을 P, 그리고 모델 예측을 Q라 두게 된다면 위 크로스 엔트로피 텀에 마이너스 부호가 붙은 것과 같게 된다
		- 최대가능도 추정법은 즉 KL divergence를 최소화 하는 것과 같다