---
layout: single
title: 'GCN paper review'
categories:
  - Graph Neural Network
---

# SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS

https://arxiv.org/pdf/1609.02907.pdf

# 1 INTRODUCTION

Graph-based regularization (Zhu et al., 2003; Zhou et al., 2004; Belkin et al., 2006; Weston et al., 2012)

![Untitled](../../assets/images/2024-02-16-GCN/Untitled.png)

L0: supervised loss (graph ë‚´ labelì´ ìˆëŠ” part)

Lreg: Laplacian matrixë¥¼ í†µí•´ ì—°ê²°ëœ nodeë“¤ ë¼ë¦¬ ì°¨ì´ê°€ ì‘ì€ ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•˜ê²Œ í•¨

**But nodeê°€ ì—°ê²°ë˜ì–´ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì´ similarityê°€ ë†’ë‹¤ëŠ” ê²ƒì„ ë³´ì¥í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— model capacityë¥¼ ë–¨ì–´ëœ¨ë¦°ë‹¤.**

<aside>
ğŸ’¡ Conditioning f(Â·)  on the adjacency matrix of the graph f(X, A) will allow the model to distribute gradient information from the supervised loss L0 and will enable it to learn representations of nodes both with and without labels.
</aside>

nnì— ì¸ì ‘í–‰ë ¬ ì§‘ì–´ë„£ì–´ì„œ graph structureë¥¼ encode í•˜ì—¬ ìœ„ì— regularization termì„ loss functionì—ì„œ ì—†ì• ê³ , labelì´ ìˆëŠ” node (supervised loss)ì— ëŒ€í•´ì„œë§Œ í•™ìŠµí•œë‹¤. ëª¨ë¸ì— ì¸ì ‘ í–‰ë ¬ ì§‘ì–´ë„£ì–´ supervised lossì˜ gradient informationì„ í˜ë ¤ë³´ë‚´ê³  ì´ëŠ” labelì´ ìˆëŠ” node ì—†ëŠ” node ëª¨ë‘ represent í•  ìˆ˜ ìˆê²Œ ë§Œë“ ë‹¤.

GCN ë…¼ë¬¸ì˜ contributionì€ í¬ê²Œ ë‘ ê°€ì§€ê°€ ì¡´ì¬í•œë‹¤.

1. NNì—ì„œì˜ layer-wiseí•œ propagation ruleì„ graphì— ì ìš©í•˜ëŠ” ë°©ë²• ì œì•ˆ
    - first-order approximation of spectral graph convolutions (Hammond et al., 2011)ì—ì„œ motivated
2. gnnì´ semi-supervised classificationì„ ì–´ë–»ê²Œ ë¹ ë¥´ê³  í™•ì¥ì„±ì´ ìˆê²Œ ì ìš©ë˜ëŠ”ì§€ ì¦ëª…
    - ì‹œê°„ê³¼ ì •í™•ë„ ì¸¡ì • ì‹¤í—˜ ì§„í–‰

# 2 FAST APPROXIMATE CONVOLUTIONS ON GRAPHS

ê·¸ëŸ¼ nnì— ì–´ë–»ê²Œ ì¸ì ‘ í–‰ë ¬ì„ ì§‘ì–´ ë„£ì„ ê±´ë°? GCNì˜ êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](../../assets/images/2024-02-16-GCN/Untitled1.png)

ì—¬ê¸°ì„œ ì¸ì ‘ í–‰ë ¬ì€ self-connected adjacency matrix ì´ë‹¤. degree of matrix ë˜í•œ ìê¸° ìì‹ ì„ í¬í•¨í•œ ì—°ê²°ëœ ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. W(l)ì€ layerë³„ í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ë‹´ê¸´ matrix(layer-specific trainable weight matrix)ë¥¼ ì˜ë¯¸í•œë‹¤.

## 2.1 SPECTRAL GRAPH CONVOLUTIONS

Graph ë‚´ Spectral convolution ì—°ì‚°ì€ Fourier domain ì—ì„œ signal x (ê° ë…¸ë“œ ë³„ scalar)ì™€ filter (theta diagonal matrix)ì˜ ê³±ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤.

![Untitled](../../assets/images/2024-02-16-GCN/Untitled2.png)

UëŠ” laplacian matrixì˜ eigenvectorë“¤ë¡œ ì´ë¤„ì§„ í–‰ë ¬ì´ê³ , graph signal xì— Uë¥¼ ê³±í•œ ê²ƒì€ graph Fourier Transform ê³¼ì •ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. g thetaëŠ” Laplacian eigenvalueë“¤ì˜ function ì—­í• ì„ í•˜ì—¬ ì ë‹¹í•œ ê°€ì¤‘ì¹˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ìœ¼ë¡œ ë³´ì—¬ì§„ë‹¤. eigenvector matrixë¥¼ multiplication í•˜ëŠ” ê²ƒì€ O(N^2)ì˜ complexityë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, eigen decomposition ë˜í•œ í° ê·œëª¨ì˜ ê·¸ë˜í”„ì—ì„œëŠ” expensiveí•˜ë‹¤.

![Untitled](../../assets/images/2024-02-16-GCN/Untitled3.png)

![Untitled](../../assets/images/2024-02-16-GCN/Untitled4.png)

![Untitled](../../assets/images/2024-02-16-GCN/Untitled5.png)

ìœ„ì™€ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ laplacian matrix eigenvalueì˜ function g thetaë¥¼ ì²´ë¹„ì…°í”„ ë‹¤í•­ì‹ì„ ì´ìš©í•˜ì—¬ Kë²ˆì§¸ orderê¹Œì§€ë§Œ ì‚¬ìš©í•˜ë„ë¡ well-approximate í•˜ì˜€ë‹¤. (Hammond et al. 2011) ê²°ê³¼ì ìœ¼ë¡œ, K-Localized filterë¥¼ êµ¬ì„±í•˜ê²Œ ë˜ì—ˆê³ , ì¤‘ì‹¬ ë…¸ë“œë¡œë¶€í„° ìµœëŒ€ Kë§Œí¼ ë–¨ì–´ì§„ ì´ì ë…¸ë“œë“¤ì—ë§Œ dependí•˜ê²Œ ë˜ì—ˆë‹¤. ì¦‰ complexityëŠ” O(|E|) edgeì˜ ê°œìˆ˜ ë§Œí¼ìœ¼ë¡œ ì¤„ì—ˆìœ¼ë©°, K-localized convolutionì„ ì •ì˜í•˜ëŠ”ë°ì— ì‚¬ìš©ë˜ì—ˆë‹¤.

## 2.2 LAYER-WISE LINEAR MODEL

ìœ„ì—ì„œ ìœ ë„ëœ ì‹ì˜ í˜•íƒœë¡œ convolutional layerë¥¼ non-linearityë¥¼ ê°€ì§„ ì±„ ìŒ“ìŒìœ¼ë¡œì¨ graph convolutionì„ í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì¡Œë‹¤.

convolution operationì„ K=1 ë¡œ ì œí•œí•œë‹¤ë©´?

ì´ëŸ¬í•œ ê²½ìš° functionì´ linear í•œ í˜•íƒœ L ì´ê³ , ì´ëŠ” graph Laplacian spectrum ë‚´ì—ì„œ linear functionì„ ì˜ë¯¸í•œë‹¤. ê·¸ëŸ°ë° ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ layerë¥¼ ìŒ“ëŠ”ë‹¤ë©´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” í˜•íƒœì˜ convolutional filter functionì„ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ì€ ë§¤ìš° ë„“ì€ node degree distiributionë“¤ì„ ê°€ì§€ëŠ” graphì—ì„œ local neighborhood êµ¬ì¡°ì— ì˜í•œ overfitting ë¬¸ì œë¥¼ í”¼í•  ìˆ˜ ìˆê²Œ í•œë‹¤. ê·¸ë¦¬ê³  ê³ ì •ëœ ê³„ì‚° ë¹„ìš©ìœ¼ë¡œ ì¸í•´, ëª¨ë¸ë¡œ í•˜ì—¬ê¸ˆ ë”ìš± ê¹Šê²Œ nnì„ êµ¬ì„±í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

![Untitled](../../assets/images/2024-02-16-GCN/Untitled6.png)

![Untitled](../../assets/images/2024-02-16-GCN/Untitled7.png)

![Untitled](../../assets/images/2024-02-16-GCN/Untitled8.png)

ìœ„ operatorì˜ ë°˜ë³µ ì ìš©ì€ dnnì—ì„œ í”íˆ ê²ªëŠ” exploding/vanishing gradient ë¬¸ì œë¥¼ ì•¼ê¸°í•˜ê¸°ì—  renormalization trickì„ í†µí•´ ì•„ë˜ì™€ ê°™ì´ ì¬êµ¬ì„±í•˜ì˜€ë‹¤.

![Untitled](../../assets/images/2024-02-16-GCN/Untitled9.png)

![Untitled](../../assets/images/2024-02-16-GCN/Untitled10.png)

![Untitled](../../assets/images/2024-02-16-GCN/Untitled11.png)

filter matrix parameterëŠ” channel ì˜ ìˆ˜ (feature vector dimension) X filter ê°œìˆ˜ë¡œ êµ¬ì„±ë˜ë©° ë§Œí¼ ìŒ“ëŠ”ë‹¤ë©´ ìµœì¢…ì ìœ¼ë¡œ O(|E|FC) ë§Œí¼ì˜ complexityë¥¼ ê°€ì§„ë‹¤.

# 3 SEMI-SUPERVISED NODE CLASSIFICATION

ê·¸ë ‡ë‹¤ë©´ f(X, A) propagation networkëŠ” ì–´ë–»ê²Œ êµ¬ì„±í•˜ì—¬ semi-supervised node classificationì„ í•˜ëŠ” ê±¸ê¹Œ?

ê·¸ë˜í”„ êµ¬ì¡°ì— ë‚´ì¬ëœ data Xì™€ adjacency matrix Aë¥¼ modelì— conditioning í•˜ì—¬ ê·¸ë˜í”„ ê¸°ë°˜ semi-supervised learning, ì¦‰ adjacency matrixë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ data Xë§Œìœ¼ë¡œ represent í•  ìˆ˜ ì—†ì—ˆë˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ìƒí™©ì„ ê¸°ëŒ€í•˜ê³  ìˆë‹¤.

semi-supervised node classificationì„ ìœ„í•œ two-layer GCN

W0: input-to-hidden weight matrix for a hidden layer (C x H)

W1: hidden-to-output weight matrix (H x F)

A: pre-processing stepì—ì„œ ë½‘ì•„ë‚´ëŠ” normalized adjacency matrix

![Untitled](../../assets/images/2024-02-16-GCN/Untitled12.png)

![Untitled](../../assets/images/2024-02-16-GCN/Untitled13.png)

![Untitled](../../assets/images/2024-02-16-GCN/Untitled14.png)

C dimì— í•´ë‹¹í•˜ëŠ” x featureì—ì„œ Hë§Œí¼ì˜ feature mapì„ ë½‘ì•„ Fê°œì˜ classì— ëŒ€í•œ í™•ë¥ ì„ êµ¬í•œë‹¤.