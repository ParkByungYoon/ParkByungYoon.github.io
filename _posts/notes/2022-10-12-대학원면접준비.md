---
layout: single
title: '대학원 면접 준비'
categories:
  - Experiment Report
---

Rudi Park님의 면접 질문 리스트를 정리해보고자 한다.

[AI 대학원 면접 준비 - 전공 질문 리스트](https://jrc-park.tistory.com/259) 보러가기

---
## *Statistic / Probability*

### 🧐 Central Limit Theorem 이란 무엇인가? 

중심극한정리는 표본의 크기가 증가함에 따라 표본 평균 분포(각각의 표본 평균들이 이루는 분포)가 정규 분포에 근사한다는 정리이다.

여기서 표본 평균 분포란  표본 크기가 n인 표본(n-30)을 여러(N)번 반복해서 추출했을 때 나오는 X1(n=30), X2(n=30), ... , XN(n=30) 각각의 표본 평균들이 이루는 분포이다.

일단 표본의 표본의 크기가 충분히 크다면, 표본평균들의 분포가 모집단의 모수를 기반으로 한 정규분포를 이룬다는 점을 이용해 특정 사건(내가 수집한 표본의 평균)이 일어날 확률을 계산할 수 있다.

즉, 모집단 분포에 상관없이 표본평균의 분포가 정규분포로 수렴한다 점을 이용하여 Z값을 구해 확률값을 구할 수 있게 된다.

### 🧐 큰수의 법칙이란?

사건을 무한히 반복할 때 일정한 사건이 일어나는 비율은 횟수를 거듭하면 할수록 일정한 값에 가까워지는 법칙이다.

어떤 모집단에서 표본들을 추출할 때, 각 표본의 크기가 커지면(시행 횟수가 늘어나면) 상대도수와 모비율의 값이 같아질 확률이 높아진다는 의미이다.

### 🧐 확률이랑 통계량 다른 점은?

통계는 표본을 통해 모집단의 모수를 추정하고 확률 모형을 결정짓는 일

통계 : 주어진 데이터 --> 모델 예측

확률은 그 확률 모형으로부터 구하는 수치이며 불확실성을 계량화하기 위한 표현

확률 : 주어진 모델 --> 데이터 예측

### 🧐 Marginal Distribution, Conditional Distribution

Marginal Probability : 개별 사건의 확률

Conditional Probability : 특정한 조건 아래 어떤 사건이 발생할 확률

### 🧐 Likelihood

Likelihood function: $L(\theta|X)$

모수를 가정해서 분포를 특정하게 되면 (θ를 특정하면) 관측한 데이터에서의 그 강도의 값을 구할 수 있게 된다.

Likelihood는 $\theta$에 대한 믿음의 강도, 값이 높으면 $\theta$에 대한 믿음의 강도가 높아진다(해당 데이터를 잘 표현하는 모수가 될 수 있겠다)

---
## *Machine Learning / Deep Learning*

### 🧐 Bias, Variance, MSE란?

Bias는 모델을 통해 얻은 예측값과 실제 정답과의 차이의 평균을 말한다. 

Variance는 다양한 데이터 셋에 대하여 예측값이 얼마나 변화할 수 있는지에 대한 양(Quantity)을 말한다.

Bias 문제는 데이터의 분포에 비하여 모델이 너무 간단한 경우 underfit이 발생한 경우에 발생합니다.

Variance 문제는 모델의 복잡도가 데이터 분포보다 커서 데이터를 overfitting 시키는 문제를 말합니다.

### 🧐 Batch Normalization이란?

네트워크의 각 레이어나 Activation 마다 입력값의 분산이 달라지는 현상을 Internal Covariance Shift이라 말한다

이러한 문제를 해결하기 위해 배치 정규화가 등장하였다. 배치 정규화는 각 레이어마다 정규화 하는 레이어를 두어, 변형된 분포가 나오지 않도록 조절한다.

미니배치의 평균과 분산을 이용해서 정규화 한 뒤에, scale 및 shift 를 감마(γ) 값, 베타(β) 값을 통해 실행한다. 이 때 감마와 베타 값은 학습 가능한 변수이다. 즉, Backpropagation을 통해서 학습이 된다. 

Infernece 시에는 미니 배치의 평균과 분산을 이용 할 수 없다. Inference 전에 미리 저장해둔 미니 배치의 이동 평균(Moving Average)을 사용하여 해결한다.

### 🧐 Graident Descent란?

함수 값이 낮아지는 방향으로 독립 변수 값을 변형시켜가면서 최종적으로는 최소 함수 값을 갖도록 하는 독립 변수 값을 찾는 방법이다.

Stochastic Gradient Desenct(SGD)는 전체 데이터에 대해 목적식을 계산해야 했을 때 너무 많은 계산량을 필요로 하므로 전체 데이터(Batch) 대신 일부 데이터의 모음(Mini-Batch)를 사용한다. 계산 속도가 훨씬 빠르기 때문에 같은 시간에 더 많은 step을 갈 수 있으며, 여러 번 반복할 경우 Batch 처리한 결과로 수렴한다. 또한 Batch Gradient Descent에서 빠질 Local Minima에 빠지지 않고 더 좋은 방향으로 수렴할 가능성도 높다. 


## *Linear Algebra*

### 🧐 Linearly Independent란?

$c_1a_1 + c_2a_2 + ... + c_na_n = 0$ 에서 만족하는 상수 $c_1,c_2, ... , c_n$ 이 모두 0이면 $a_1, a_2, ... , a_n$는 선형 독립(linearly independent)이다.

그렇지 않고 $c_1,c_2, ... , c_n$ 중 0이 아닌 $c_i$가 존재한다면 선형 종속(linearly dependent)이다.

### 🧐 Basis란?

기저(basis, 基底)란 벡터 공간의 임의의 원소를 표현하기 위해 필요한 최소한의 벡터로 이루어진 집합이다.

$y_1, y_2, ... + y_3 =c_1a_1 + c_2a_2 + ... + c_na_n$의 해($c_1,c_2, ... , c_n$)가 1조만 존재하는 경우 $a_1, a_2, ... , a_n$ 의 집합을 기저라고 한다.

기저가 아닌 예

1. 2차원의 임의의 원소를 표현하는데 필요한 최소한의 벡터는 2개이다. 하지만 2개 이상의 벡터로 구성되어있을 경우 2차원의 임의의 원소를 표현하기 위해 필요한 최소의 벡터로 이루어진 집합인 기저(basis)가 아닌 것이 된다.

2. 해가 아예 하나도 존재하지 않기 때문에 기저가 아닌 경우에도 기저가 아닌 예 중 하나이다. 벡터 중 n번째 원소가 모두 '0'으로 되어 있어서 n번째 차원을 표현할 방법이 없으므로 기저가 아니다.

3. 3차원의 임의의 원소를 표현하기 위한 최소한의 벡터 개수는 3개이다. 하지만 벡터가 2개 밖에 없어서 1개가 모자란 경우 세번째 차원의 원소를 표현할 방법이 없으므로 기저의 정의를 만족시키지 못한다.

### 🧐 Dimension, Rank, Span

dimension: 벡터 공간을 표현하는데 필요한 기저벡터의 개수

span: 벡터를 통해 부분공간을 형성하는 것

rank: 행렬에서 서로 독립인 열 벡터의 개수


### 🧐 Eigen Vector, Eigen Value

정방행렬 A에 대하여 Ax = λx  (상수 λ) 가 성립하는 0이 아닌 벡터 x가 존재할 때 상수 λ 를 행렬 A의 고유값 (eigenvalue), x 를 이에 대응하는 고유벡터 (eigenvector) 라고 한다.  

이는 벡터 x에 대해 n차 정방행렬 A를 곱하는 결과와 상수 λ를 곱하는 결과가 같다는 걸 의미한다. 즉, 행렬의 곱의 결과가 원래 벡터와 "방향"은 같고, "배율"만 상수 λ 만큼만 비례해서 변한다. eigenvector의 방향은 똑같고 , 크기만 eigenvalue 만큼씩 배수(magnification)가 된다.