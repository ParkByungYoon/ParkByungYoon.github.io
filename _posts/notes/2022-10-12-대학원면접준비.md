---
layout: single
title: '대학원 면접 준비'
categories:
  - Experiment Report
---

Rudi Park님의 면접 질문 리스트를 정리해보고자 한다.

[AI 대학원 면접 준비 - 전공 질문 리스트](https://jrc-park.tistory.com/259) 보러가기

---
## *Statistic / Probability*

### 🧐 Central Limit Theorem 이란 무엇인가? 

중심극한정리는 표본의 크기가 증가함에 따라 표본 평균 분포(각각의 표본 평균들이 이루는 분포)가 정규 분포에 근사한다는 정리이다.

여기서 표본 평균 분포란  표본 크기가 n인 표본(n-30)을 여러(N)번 반복해서 추출했을 때 나오는 X1(n=30), X2(n=30), ... , XN(n=30) 각각의 표본 평균들이 이루는 분포이다.

일단 표본의 표본의 크기가 충분히 크다면, 표본평균들의 분포가 모집단의 모수를 기반으로 한 정규분포를 이룬다는 점을 이용해 특정 사건(내가 수집한 표본의 평균)이 일어날 확률을 계산할 수 있다.

즉, 모집단 분포에 상관없이 표본평균의 분포가 정규분포로 수렴한다 점을 이용하여 Z값을 구해 확률값을 구할 수 있게 된다.

### 🧐 큰수의 법칙이란?

사건을 무한히 반복할 때 일정한 사건이 일어나는 비율은 횟수를 거듭하면 할수록 일정한 값에 가까워지는 법칙이다.

어떤 모집단에서 표본들을 추출할 때, 각 표본의 크기가 커지면(시행 횟수가 늘어나면) 상대도수와 모비율의 값이 같아질 확률이 높아진다는 의미이다.

### 🧐 확률이랑 통계량 다른 점은?

통계는 표본을 통해 모집단의 모수를 추정하고 확률 모형을 결정짓는 일

통계 : 주어진 데이터 --> 모델 예측

확률은 그 확률 모형으로부터 구하는 수치이며 불확실성을 계량화하기 위한 표현

확률 : 주어진 모델 --> 데이터 예측

### 🧐 Marginal Distribution, Conditional Distribution

Marginal Probability : 개별 사건의 확률

Conditional Probability : 특정한 조건 아래 어떤 사건이 발생할 확률

### 🧐 Likelihood

Likelihood function: $L(\theta|X)$

모수를 가정해서 분포를 특정하게 되면 (θ를 특정하면) 관측한 데이터에서의 그 강도의 값을 구할 수 있게 된다.

Likelihood는 $\theta$에 대한 믿음의 강도, 값이 높으면 $\theta$에 대한 믿음의 강도가 높아진다(해당 데이터를 잘 표현하는 모수가 될 수 있겠다)

---
## *Machine Learning / Deep Learning*

### 🧐 Bias, Variance, MSE란?

Bias는 모델을 통해 얻은 예측값과 실제 정답과의 차이의 평균을 말한다. 

Variance는 다양한 데이터 셋에 대하여 예측값이 얼마나 변화할 수 있는지에 대한 양(Quantity)을 말한다.

Bias 문제는 데이터의 분포에 비하여 모델이 너무 간단한 경우 underfit이 발생한 경우에 발생합니다.

Variance 문제는 모델의 복잡도가 데이터 분포보다 커서 데이터를 overfitting 시키는 문제를 말합니다.

### 🧐 Batch Normalization이란?

네트워크의 각 레이어나 Activation 마다 입력값의 분산이 달라지는 현상을 Internal Covariance Shift이라 말한다

이러한 문제를 해결하기 위해 배치 정규화가 등장하였다. 배치 정규화는 각 레이어마다 정규화 하는 레이어를 두어, 변형된 분포가 나오지 않도록 조절한다.

미니배치의 평균과 분산을 이용해서 정규화 한 뒤에, scale 및 shift 를 감마(γ) 값, 베타(β) 값을 통해 실행한다. 이 때 감마와 베타 값은 학습 가능한 변수이다. 즉, Backpropagation을 통해서 학습이 된다. 

Infernece 시에는 미니 배치의 평균과 분산을 이용 할 수 없다. Inference 전에 미리 저장해둔 미니 배치의 이동 평균(Moving Average)을 사용하여 해결한다.

### 🧐 Graident Descent란?

함수 값이 낮아지는 방향으로 독립 변수 값을 변형시켜가면서 최종적으로는 최소 함수 값을 갖도록 하는 독립 변수 값을 찾는 방법이다.

Stochastic Gradient Desenct(SGD)는 전체 데이터에 대해 목적식을 계산해야 했을 때 너무 많은 계산량을 필요로 하므로 전체 데이터(Batch) 대신 일부 데이터의 모음(Mini-Batch)를 사용한다. 계산 속도가 훨씬 빠르기 때문에 같은 시간에 더 많은 step을 갈 수 있으며, 여러 번 반복할 경우 Batch 처리한 결과로 수렴한다. 또한 Batch Gradient Descent에서 빠질 Local Minima에 빠지지 않고 더 좋은 방향으로 수렴할 가능성도 높다. 