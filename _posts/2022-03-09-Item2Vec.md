---
layout: single
title: 'Recommendation System Item2Vec'
---

## Item2Vec
- Embedding
  - 주어진 데이터를 낮은 차원의 벡터로 만드는 방법
  - Sparse Representation
    - 아이템 전체 가짓수와 차원의 수가 동일
  - Dense Representation
    - 아이템의 전체 가짓수보다 훨씬 작은 차원으로 표현 가능
- Word Embedding
  - 텍스트 분석을 위해 단어를 벡터로 표현
  - 단어 간 의미적인 유사도를 구할 수 있다
  - 임베딩으로 표현하기 위해선 학습 모델이 필요
    - MF의 유저/아이템 parameter matrix가 곧 임베딩
- Word2Vec
  - Neural Network 기반
  - 대량의 문서를 vector 공간에 투영
  - 압축된 형태의 dense vector
  - 효율적이고 빠른 학습이 가능
  - 세가지 학습 방법
    - CBOW
    - Skip Gram
- Continuous Bag of Words (CBOW)
  - 주변 단어를 가지고 센터에 있는 단어를 예측
  - 앞뒤 n개의 단어를 사용해 예측
  - 총 두가지의 파라미터
    - Input에서 Dense representation(embedding matrix/lookup table)으로 가는 Parameter W
    - Dense representation에서 Output(Sparse representation)으로 가는 Parameter W'
  - 각 Input과 Parameter W를 곱하고 평균내어 embedding matrix 구성
  - embedding matrix와 W'를 곱한 뒤 softmax를 통해 확률값으로 표현 (Multi-Classification)
- Skip-Gram
  - CBOW의 입력층과 출력층이 반대
  - 벡터의 평균을 구하는 과정 x (하나의 input만이 들어가기 떄문)
  - 일반적으로 Skip-Gram의 표현력이 CBOW보다 좋다고 알려져있음
  - 하나의 Input 주위에 있을만한 단어들을 예측 (Multi-Classification)
- Skip-Gram with Negative Sampling (SGNS)
  - 입력으로 두 개의 단어 (중심단어, 주변단어)
  - 주변에 있는 단어인지 아닌지를 판별하는 Binary Classification
  - Skip-Gram의 경우 주변에 있지 않은 데이터의 경우 학습 데이터로 사용 x
    - 이러한 이유로 SGNS는 주변에 있지 않은(negative) 데이터를 샘플링한다
  - 중심 단어와 주변 단어는 서로 다른 lookup table으로 임베딩
  - 두 단어를 내적한 뒤 sigmoid를 씌워 예측값을 내놓는다
  - backpropagation을 통해 각 임베딩이 업데이트 되며 모델이 수렴
  - 최종 생성된 워드 임베딩 2개 중 하나만 사용하거나 2개의 평균을 사용하기도 한다
- Item2Vec
  - 아이템 기반 CF에 Word2Vec을 사용
  - 단어가 아닌 추천 아이템을 Word2Vec을 사용해 임베딩
    - 유저가 소비한 아이템 리스트 : 문장
    - 아이템 : 단어
  - Item2Vec은 유저-아이템 관계 사용 x, 유저가 사용한 아이템만을 학습 데이터로 사용 (유저 식별 없이 세션 단위로도 데이터 생성 가능)
  - SGNS 기반의 Word2Vec을 사용해 아이템을 벡터화
  - 유저 혹은 세션 별로 소비한 아이템 집합을 생성
    - 시퀀스를 집합으로 바꿔 공간적/시간적 정보 사라진다
    - 집합 안에 존재하는 아이템은 유사하다고 가정
  - 동일한 아이템 집합 내 아이템 쌍들은 SGNS의 Postive Sample이 된다
    - 모든 조합 고려 (집합 {A,B,C} 일때 AB, AC, BA, BC, CA, CB 모두 positive sampling)
  - Item2Vec vs SVD(MF)
    - t-SNE(차원 축소 기법)로 임베딩하여 시각화