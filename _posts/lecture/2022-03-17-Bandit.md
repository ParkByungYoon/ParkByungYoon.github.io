---
layout: single
title: 'Multi-Armed Bandit'
categories:
  - boostcamp AI tech
---
## Multi-Armed Bandit (MAB)

카지노에 있는 슬롯머신(=One-Armed Bandit)은 한 번에 한 개의 arm을 당길 수 있다 카지노에 있는 K개의 슬롯머신을 N번 플레이 할 수 있다면 (Multi-Armed Bandit) K개의 슬롯머신에서 얻을 수 있는 reward의 확률이 모두 다르다고 가정 했을 때 수익을 최대화하기 위해선 arm을 어떤 순서대로 혹은 어떤 정책(policy)에 의해 당겨야 할까?

우리는 슬롯머신의 reward 확률을 정확히 알수 없기 때문에 Exploration과 Exploitation 간 Trade off가 발생한다 
    
    MAB 정책 예시
    1. 모든 슬롯머신을 동일한 횟수로 당긴다
    - 높은 reward를 기대하기 어렵다
    - 다양한 슬롯머신을 돌려보며 정확한 예측값을 얻는 행위 Exploration

    2. 일정횟수만큼 슬롯머신을 당겨보고 남은 횟수는 그 시간 동안 제일 높은 확률을 가졌던 슬롯머신만을 당긴다
    - 동일한 슬롯만 계속 당기게 될 수 있다
    - Reward가 큰 슬로머신만을 반복적으로 돌리는 행위 Exploitation

정리하면 exploration은 더 많은 정보를 얻기 위해 새로운 arm을 탐색하는 것을 말하며 exploitation은 기존의 경험 혹은 관측을 토대로 가장 좋은 arm을 선택하는 것을 말한다 exploration(탐색)에 비용을 지나치게 낭비한다면 높은 reward를 얻을 수 없으며 exploitation(활용)이 많은 경우에는 reward를 보장해주지 않는다 이러한 trade-off 가운데 적정 지점을 잘 찾는 것이 중요하다

모든 action에 대한 추정치를 알고 있다면 문제가 해결되겠지만 reward의 true distribution을 우리는 모르기 때문에 추정해야하며 그에 대한 시간 t에서의 추정치를 최대한 정밀하게 구하는 것이 목표이다 

1. Greedy Algorithm

    실제 기대값의 가장 간단한 추정 방식으로 표본 평균을 사용한다 (ex. 지금까지 관측된 개별 머신의 reward 평균값)

    가장 간단한 policy로 평균 reward가 최대인 action을 선택한다(Greedy)

    하지만 policy가 처음에 선택되는 action과 reward에 크게 영향을 받는다 (exploration이 부족하다)

2. Epsilon-Greedy Algorithm 
    
    exploration이 부족한 Greedy algorithm의 policy를 수정한 전략이다

    일정한 확률에 의해 랜덤으로 슬롯머신을 선택하도록 한다 (ex. 동전을 던져 앞면이 나오면 greedy, 뒷면이 나오면 random 선택(epsilon))

    Epsilon-Greedy Algorithm는 exploration과 exploition을 항상 어느정도 보장하는 간단하지만 강력한 알고리즘이다.

    하지만 time step이 어느정도 지나고 나서는 true distribution을 추정이 가능한 상황이 오기 때문에 무조건적으로 random하게 적용하는 Epsilon-Greedy Algorithm의 성능은 비교적 떨어진다

3. Upper Confidence Bound (UCB)

    UCB는 simple average에 하나의 term이 추가되었다. 새로 추가된 term은 해당 action이 최적의 action이 될 수도 있는 가능성(불확실성)을 의미하여 관측치가 적을 때 더 높은 값을 가진다 그리고 해당 term의 가중치 하이퍼파라미터 c 또한 존재하여 exploration 조정이 가능하다

4. MAB를 이용한 추천 예시

    Bandit 문제로 바꾼다면 실제 서비스의 지표인 클릭/구매를 모델의 reward로 가정하며 해당 reward를 최대화하는 방향으로 모델이 학습되고 추천을 수행한다 이를 통해 무거운 추천 모델을 사용하지 않아도 되고 간단한 Bandit 기법을 적용하여 온라인 지표를 높일 수 있다

        추천하는 개별 아이템 = 개별 action
        유저에게 추천하는 방식 = MAB policy
        아이템 추천 시 클릭 여부 = reward
        exploration = 지속적으로 변화하는 유저의 취향 탐색 및 추천 아이템 확장
        exploitation = 유저의 취향에 맞는 아이템 추천

    유저 추천 시에는 개인별 데이터가 부족해 Bandit이 수렴하지 않아 개별 유저에 대해 모든 아이템의 Bandit을 구하는 것은 불가능하다 이러한 이유로 클러스터링을 통해 비슷한 유저끼리 그룹화한 뒤 그룹내에서 Bandit을 구추한다 (Bandit의 개수 = 클러스터 개수 * 후보 아이템 개수)

    유사 아이템 추천 시에는 유사한 후보 아이템 리스트를 생성하고 그 안에서 Bandit을 적용한다 유사한 아이템 리스트는 MF, Item2Vec 기반의 유저-아이템 상관관계를 기반으로 한 유사도나 content-based 유사도를 구해 추출한다 (Bandit의 개수 = 아이템 개수 * 후보 아이템 개수)