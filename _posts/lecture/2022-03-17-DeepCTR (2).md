## DeepCTR (2)

1. Deep Interest Network (DIN)
    
    User Behavior feature를 처음 사용한 논문
    
    기존의 딥러닝 기반 모델들은 대부분 sparse feature들을 저차원 임베딩으로 변환 후 MLP 입력으로 사용하는 Embedding - MLP 패러다임을 따라 사용자의 다양한 관심사를 반영하기에 어려움이 있었다 예를 들어 여러 종류의 식재료와 생필품을 동시에 찾아 볼 때나 특정 카테고리의 상품을 검색해 보던 중 추천 목록에 있는 상품을 클릭할 때와 같이 다양한 아이템이 동시에 소비 될 수 있는 상황에 대해 고려하지 못했다.

    이를 해결하기 위해 논문에서는 사용자가 기존에 소비한 아이템 리스트를 User Behavior Feature를 만들어 예측 대상 아이템과 이미 소비한 아이템 사이에 관련성을 학습하고자 하였다

    DIN 모델의 구조는 다음과 같다

       1. Embedding Layer 
       2. Local Activation Layer
       3. Fully-connected Layer 
    
    위 구조 중 Local Activation Layer에서 multi-hot 형태인 user behavior feature와 노출하고자 하는 아이템 간의 관련성을 학습한다

    현재 노출하고자하는 Candidate Ad와 각 과거 소비했던 아이템들을 pair로 activation unit으로 들어감을 통헤 임베딩된 값이 scalar 값으로 추출하는 과정을 거친다 과거에 소비한 아이템과 현재 예측하려는 아이템 사이 얼마나 연관이 있는지 계산하는 이 값을 논문에서는 "Activation Weight"라 한다 즉 Weight가 높을 수록 높게 반영하며(**transformer attention 메커니즘과 유사하다**) 구해진 weight들은 sum pooling을 통해 차원이 늘어나지 않고 같은 차원으로 유지되도록 한다

2. Behavior Sequence Transformer(BST)

    Transformer를 사용하여 CTR을 예측한 논문

    CTR 예측 데이터와 NLP 번역 데이터 간에는 공통점이 존재한다 1. 대부분 sparse feature로 구성되어있고 2. low/high-order feature interaction이 모두 존재해 비선형적인 관계를 이루며 3. NLP에서 문장 순서가 중요하듯 사용자의 행동 순서 또한 중요하게 작용한다는 점이 있다(ex. 핸드폰 구매 후 핸드폰 케이스 상품을 찾는 것) 이러한 NLP 데이터에 강력한 성능을 보이는 Transformer 구조를 CTR 예측에 적용했다

    **Attention is All You Need**
    
    Transformer를 짧게 리뷰해보자면 우리는 Attention Mechanism에 대해 살펴볼 필요가 있다 Attention Mechanism에은 말 그대로 어떤 부분에 주의(Attention)을 기울일 것인지를 찾는 원리를 말한다 key-value 쌍들이 주어졌을 때 알고자하는 query에 대응되는 value를 query와 key들의 연관성을 가중치로 하여 value들의 가중 합으로 계산한다

        Attention(Q,K,V) 
        = Value of Q
        = similarity(Q,K) * V
    
    또 Attention Mechanism을 사용한다면 입출력의 길이를 고려하지 않아도 단어간 의존성 파악이 가능하다는 장점이 존재한다

    NLP에서는 주어진 단어(=query)가 전체 단어(=keys)와 얼마나 관계가 있는지를 가중치로 사용하여 전체 단어에 대응되는 값들(=values)의 가중 합으로 주어진 단어에 대응되는 결과를 도출한다

    Scale Dot-product Attention은 query와 key 벡터를 내적한 값을 key의 차원 크기(d_k)에 비례하게 스케일링하고 softmax 함수에 넣어 query와 key 사이의 유사도로 취급한다 Scale Dot-product Attention은 self-attention에 해당하며 여기서 self-attention은 query, key value가 모두 같은 도메인에 속하는 attention을 말한다

    Multi-head Attention은 차원이 큰 attention을 한번 수행하는 것보다 여러개의 작은 attention을 병렬로 처리하는 것을 말한다 즉 여러개의 Scale Dot-product Attention을 각각 Head로 두어 표현한다

    Transformer는 Encoder-Decoder 구조를 가지며 각각 6개의 동일한 레이어를 쌓아 구성되었다 레이어는 Add & Norm, MHA 등으로 구성되어있으며 Positional Encoding을 통해 단어가 등장하는 위치 정보를 피쳐 임베딩에 더해 줌으로서 단어의 순서를 표현하였다
    
    BST의 전체 구조는 어떨까? 우선 User Behavior를 Sequence로 구성하여 Input으로 사용하였다 User Behavior Sequence와 Target Item을 한번에 묶어 Transformer Layer에 사용되고 다른 feature의 경우에도 embedding을 거쳐 사용된다 Transformer Layer는 기존 Trasformer 모델에서 Decoder를 제외하고 Encoder 부분만을 사용하여 구성되었다

    BST는 DIN과 비교했을 때 local activation layer를 통해 user behavior를 학습한것과는 달리 순서를 반영하기 위해 transformer laye를 통해 user behavior sequence를 사용했다는 차이점이 존재한다

    또 BST와 Transformer와의 차이점에는 1. ropout과 leakyReLU를 사용했다는 점과 2. 레이어 수가 1~4개 정도였으며 (Trasnformer 블록을 2개 이상 쌓을 때 오히려 성능이 감소함, CTR task의 sequence는 NLP보다 덜 복잡한 것으로 보여짐) 3. positional encoding 방법으로 싸인, 코싸인을 사용한 기존 trasnformer와 달리 현재 추천하고자하는 시각과 아이템을 소비한 물리적인 시각 간의 차이를 사용했다는 점이 있다
