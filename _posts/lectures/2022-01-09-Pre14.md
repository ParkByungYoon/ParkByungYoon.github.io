---
layout: single
title: "딥러닝 학습 방법"
categories:
  - boostcamp AI tech
---

## 신경망 (Neural Network)

- 비선형 모델 (선형 모델 + 비선형 함수)
- 각 행벡터 **o**는 데이터 **x**와 가중치 행렬 **W** 사이의 행렬곱과 절편 **b** 벡터의 합으로 표현
- d개의 변수로 p개의 선형모델을 만들어 p개의 잠재변수를 설명하는 모델 (**W** : d X p)
- 분류 문제 (어떤 클래스에 해당하는가)
  - softmax 함수
    - 모델의 출력을 확률로 해석할 수 있게 변환
    - 벡터를 확률벡터로 변환 [1,2,0] --> [0.24, 0.67, 0.09]
    - 추론 시에는 사용하지 않는다
- 활성함수(activation function)
  - 입력값: 벡터 x, 실수값 o
  - 비선형 함수로서 딥러닝에서 매우 중요, ReLU 함수가 주로 사용됨
- 선형 모델과 활성함수를 반복적으로 사용
  - 2-layers --> 두개의 가중치 행렬
  - multi-layer는 신경망이 여러층 합성된 함수
    - L개의 가중치 행렬 W 와 절편 b로 이루어져있다
- 순전파(Forward-propagation)
  - 학습 x, 순전히 output을 수식을 통해 뽑는 과정 
- 왜 층을 여러개 쌓는가?
  - 2층 신경망으로도 임의의 연속함수 근사가 가능 (universal approximation theorem)
  - **But** 목적함수를 근사하는데 필요한 뉴런의 숫자가 훨씬 빨리 줄어들어 효율적인 학습 가능 
    - 층이 깊어지면 적은 parameter 숫자로도 근사가 가능해진다

- 역전파(Back-propagation)

  - 각 층이 가진 가중치 행렬 W에 대해서 Loss function에 대한 미분을 계산
  - 위층부터 역순으로 gradient vector를 전달 (연쇄법칙)
  - 합성함수 미분법 연쇄법칙(chain-rule) 기반 자동 미분법(auto-differentiation)을 사용

  ![jpg](/assets/images/2022-01-09/20220109_165824.jpg)

  - 각 텐서 값을 메모리에 저장되어야 미분 계산이 가능
